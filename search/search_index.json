{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"linear_models/linear_classification/","title":"Linear Classification","text":""},{"location":"linear_models/linear_classification/#logistic-regression","title":"Logistic Regression","text":"<p>Due to ease and high computational speed of [[#Linear Regression]] it is desirable to apply the same approach to classification tasks.</p> <p>Logistic Regression outputs the logistic of the result:</p> <p>$$ \\hat{p}=\\sigma(\\boldsymbol{\\theta}^T\\boldsymbol{x}) $$</p> <p>where $\\sigma$ is a standard logistic function (or sigmoid)</p> <p>$$ \\sigma(t)=\\frac{1}{1 + \\exp{(-t)}} $$</p> <p>Thus, the output range is scaled to $[0;1]$ range of probability of a particular instance being positive ($y=1$).</p> <p>Output of the original regression equation can be expressed as a logit function (or logarithm of the odds, which is the inverse of the logistic function):</p> <p>$$ \\boldsymbol{\\theta}^T\\boldsymbol{x}=\\sigma^{-1}(p)=\\ln{(\\text{odds}(\\hat{p}))}=\\ln{\\left(\\frac{p}{1-p}\\right)} $$</p> <p>The cost function for classification tasks is known as cross entropy loss (or log loss). In case for binary classification it\u2019s binary cross entropy</p> <p>$$ J(\\boldsymbol{\\theta})=-\\frac{1}{m}\\sum_{i=1}^m{\\left(y^{(i)}\\log{\\hat{p}^{(i)}}+\\left(1-y^{(i)}\\right)\\log{\\left(1-\\hat{p}^{(i)}\\right)}\\right)} $$</p> <p>Note</p> <p>It makes sense because either when $y\\rightarrow 1,\\hat{p}\\rightarrow 0$ or $y\\rightarrow 0,\\hat{p}\\rightarrow 1$ the penalty ($-\\log{\\hat{p}}$ or $-\\log{\\left(1-\\hat{p}\\right)}$, respectively) gets very large.</p> <p>There is no known closed-form solution to compute the value of $\\boldsymbol{\\theta}$ that minimizes binary cross entropy.</p> <p>However, just like MSE, cross entropy is a convex function. Thus Batch Gradient Descent is guaranteed to find the optimal solution within the specified tolerance value.</p> <p>The gradient vector $\\nabla_{\\boldsymbol{\\theta}}{J(\\boldsymbol{\\theta})}$ is just like for $\\text{MSE}$ (here) and given by</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{J(\\boldsymbol{\\theta})}= \\frac{2}{m}\\boldsymbol{X}^T\\left(\\sigma\\left({\\boldsymbol{X}\\boldsymbol{\\theta}}\\right)-\\boldsymbol{y}\\right) $$</p> <p>Code:  <code>sklearn.linear_model.LogisticRegression</code> (comes with $\\ell_2$ penalty switched on by default. Adjust <code>C</code> hyperparameter to control the regularization strength).</p> <p>Warning</p> <p>Categorical variables for logistic regression must be one-hot encoded omitting one of the classes, just like in case of [[#Linear Regression]]</p>"},{"location":"linear_models/linear_classification/#softmax-regression","title":"Softmax Regression","text":"<p>The Logistic Regression model can be generalized to support multiple classes directly, without having to train [[Classification#One-Versus-the-Rest OvR|OvR]] or [[Classification#One-Versus-One OvO|OvO]] classifiers. The resulting probability of instance belonging to the class $k$ is given by</p> <p>$$ \\hat{p}k=\\text{softmax}(\\boldsymbol{s})_k=\\frac{\\exp{(s_k)}}{\\sum{i=1}^l{\\exp{(s_i)}}} $$</p> <p>where $\\boldsymbol{s}$ is a vector containing scores of each class $k$ for the instance $\\boldsymbol{x}$</p> <p>$$ \\boldsymbol{s}=\\boldsymbol{\\Theta}\\boldsymbol{x} $$</p> <p>Now each class has its own dedicated parameter vector $\\boldsymbol{\\theta}^{(k)}$. All together, class-wise parameter vectors form a parameter matrix $\\boldsymbol{\\Theta}$.</p> <p>As for [[Classification#Binary classification|Binary Classification]], the cost function used is cross entropy yet in a generalized version known as categorical cross entropy:</p> <p>$$ J(\\boldsymbol{\\Theta})=-\\frac{1}{m}\\sum_{i=1}^m{\\sum_{k=1}^K{y_k^{(i)}\\log{\\left(\\hat{p}_k^{(i)}\\right)}}} $$</p> <p>The gradient vector with regard to $\\boldsymbol{\\theta}^{(k)}$ parameter vector $\\nabla_{\\boldsymbol{\\theta}^{(k)}}{J(\\boldsymbol{\\theta})}$ is just like for $\\text{MSE}$ (here) and given by</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}^{(k)}}{J(\\boldsymbol{\\Theta})}= \\frac{2}{m}\\boldsymbol{X}^T\\left(\\text{softmax}\\left({\\boldsymbol{X}\\boldsymbol{\\theta}}\\right)-\\boldsymbol{y}\\right) $$</p> <p>code <code>scikit.learn.LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")</code> (by default <code>LogisticRegression</code> uses [[Classification#One-Versus-the-Rest OvR|OvR]] wrapper for multiclass tasks. Comes with $\\ell_2$ penalty switched on by default. Adjust <code>C</code> hyperparameter to control the regularization strength))</p>"},{"location":"linear_models/linear_regression/","title":"Linear Models","text":""},{"location":"linear_models/linear_regression/#linear-regression","title":"Linear Regression","text":"<p>Linear Regression is defined by</p> <p>$$ \\hat{y} = \\boldsymbol{\\theta}^T \\boldsymbol{x}, $$</p> <p>where $\\hat{y}$ represents the predicted value, $\\boldsymbol{\\theta}$ is a $(n, 1)$ vector of model weights, $\\boldsymbol{x}$ is a $(n, 1)$ vector of features, and $n$ is the number of features.</p> <p>The mean squared error $\\text{MSE}(\\boldsymbol{\\theta})$ is commonly used as the cost function:</p> <p>$$ \\text{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m{(\\boldsymbol{\\theta}^T \\boldsymbol{x}_i - y_i)^2} = \\frac{1}{m}||\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}||_2^2, $$</p> <p>where $m$ represents the number of training samples, $\\boldsymbol{X}$ is an $(m, n)$ matrix containing the training samples.</p> <p>Note</p> <p>The $|| x ||_2$ notation represents the Euclidean (or L2) norm of the vector. Similarly, $|| x ||_2^2$ represents the squared Euclidean norm.</p> <p>There are 2 main solution approaches:</p> <ul> <li>Closed-form solution: This approach computes the optimal parameters directly.</li> <li>Gradient descent (GD): GD iteratively finds nearly optimal parameters by fitting either the entire training set or its batches.</li> </ul>"},{"location":"linear_models/linear_regression/#closed-form-solution","title":"Closed-Form Solution","text":"<p>To minimize the cost function $\\text{MSE}(\\boldsymbol{\\theta})$, we seek the point at which its gradient vanishes:</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})}=0, $$</p> <p>$$ \\frac{1}{m}\\nabla_{\\boldsymbol{\\theta}} |\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}|_2^2=0. $$</p> <p>Considering that $|\\boldsymbol{X}|_2^2=\\boldsymbol{X}^T\\boldsymbol{X}$, we have:</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{(\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y})^T(\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y})}=0, $$</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{(\\boldsymbol{\\theta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^T\\boldsymbol{X}^T\\boldsymbol{y}-\\boldsymbol{y}^T\\boldsymbol{X}\\boldsymbol{\\theta}+\\boldsymbol{y}^T\\boldsymbol{y})}=0. $$</p> <p>Utilizing an unidentified scalar triple product property:</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{(\\boldsymbol{\\theta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}-2\\boldsymbol{\\theta}^T\\boldsymbol{X}^T\\boldsymbol{y}+\\boldsymbol{y}^T\\boldsymbol{y})}=0. $$</p> <p>Applying the gradient:</p> <p>$$ 2\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}-2\\boldsymbol{X}^T\\boldsymbol{y}=0, $$</p> <p>$$ \\boxed{\\boldsymbol{\\theta}=(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}}. $$</p> <p>The resultant equation is recognized as the normal equation.</p> <p>Computational complexity</p> <p>Because of computational complexity and fitting the entire dataset in memory, closed-form solution can be unsuitable for large $m$ and $n$ (i.e. large number of instances and features, respectively).</p>"},{"location":"linear_models/linear_regression/#gradient-descent","title":"Gradient Descent","text":"<p>Gradient descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems.</p> <p>Mind scaling</p> <p>Features must be put onto the same scale for good convergence.</p> <p>Gradient descent begins with random initialization of model parameters and tweaks the model\u2019s parameter vector $\\boldsymbol{\\theta}$ opposite to the gradient of the cost function at each iteration</p> <p>$$ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})} $$</p> <p>where $\\eta$ is the learning rate.</p> <p>The gradient vector $\\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})}$ is given by</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})}= \\begin{pmatrix} \\frac{\\partial}{\\partial \\theta_0} \\text{MSE}(\\boldsymbol{\\theta}) \\ \\vdots \\ \\frac{\\partial}{\\partial \\theta_n} \\text{MSE}(\\boldsymbol{\\theta}) \\end{pmatrix} = \\frac{2}{m}\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}) $$</p> <p>(it was derived here). Thus, partial derivatives are given by</p> <p>$$ \\frac{\\partial}{\\partial \\theta_j}\\text{MSE}(\\boldsymbol{\\theta})=\\frac{2}{m}\\sum_{i=1}^m{\\left(\\boldsymbol{\\theta}^T\\boldsymbol{x}^{(i)}-y^{(i)}\\right)x_j^{(i)}} $$</p> <p>The iterations continue until the norm of the gradient becomes less than the specified tolerance value $\\epsilon$</p> <p>$$ |\\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})}|&lt;\\epsilon $$</p> <p>The cost function $\\text{MSE}(\\boldsymbol{\\theta})$ is convex, meaning that gradient descent is guaranteed to find the global minimum within the specified tolerance.</p> <p>Note</p> <p>It can be shown that for a convex function and constant learning rate batch gradient descent time is $O(1/\\epsilon)$.</p> <p>There\u2019s 3 types of Gradient Descent:</p> <ul> <li>Batch gradient descent (shown above): the entire training set is used to compute the gradient. Guaranteed to find optimal parameters for convex cost functions. Gets slow with large datasets and lots of features and does not support out-of-core computations.</li> <li>Mini-batch gradient descent: a single batch is used to compute the gradient. Scale well to huge datasets and supports out-of-core computations.</li> <li>Stochastic gradient descent: a single instance is used to compute the gradient. May require learning rate schedule for good convergence. Scale well to huge datasets and supports out-of-core computations.</li> </ul> <p>Complexity is $O(m\\times n)$.</p>"},{"location":"linear_models/linear_regression/#examples","title":"Examples","text":""},{"location":"linear_models/linear_regression/#closed-form-solution-with-numpy","title":"Closed-Form Solution with NumPy","text":"<p>Linear regression of a single feature synthetic dataset with noise.</p> <p>The input data is given by</p> <pre><code>import numpy as np\n\nsize = 100\n\nnoise = 0.5 * np.random.randn(size, 1)\nX = 2 * np.random.rand(size, 1)\ny = 3 * x + 4 + noise\n</code></pre> <p>Solution</p> <pre><code>import numpy as np\n\ntheta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n</code></pre>"},{"location":"linear_models/linear_regression/#closed-form-solution-with-scikit-learn","title":"Closed-Form Solution with scikit-learn","text":"<p>The problem statement is the same as Closed-Form Solution by Hand example.</p> <p>Solution</p> <pre><code>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X, y)\nmodel.intercept_, model.coef_\n</code></pre>"},{"location":"linear_models/linear_regression/#different-types-of-gradient-descent","title":"Different Types of Gradient Descent","text":"<p>Batch gradient descent is not available in scikit-learn. However, it\u2019s easy to implement:</p> <pre><code>import numpy as np\n\neta = 0.1\nmax_iter = 1000\nm, n = X.shape\n\ntheta = np.random.randn(n, 1)\n\nfor _ in range(max_iter):\n    grad = 2 / m * np.dot(X.T, np.dot(X, theta) - y)\n    theta -= eta * grad\n</code></pre> <p>Stochastic gradient descent is available directly:</p> <pre><code>from sklearn.linear_model import SGDRegressor\n\nmodel = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\nmodel.fit(X, y)\n\nmodel.intercept_, model.coef_\n</code></pre> <p>Mini-batch gradient descent is not available in scikit-learn.</p>"},{"location":"linear_models/linear_regression/#other-types-of-regression","title":"Other types of regression","text":""},{"location":"linear_models/linear_regression/#polynomial-regression","title":"Polynomial Regression","text":"<p>Linear models, such as <code>LinearRegression</code> (<code>LogisticRegression</code>), <code>SGDClassifier</code> (<code>SGDRegressor</code>) can be extended to higher powers of input features with <code>PolynomialFeatures</code> from <code>sklearn.preprocessing</code>. It returns all possible permutations of a given degree, e.g. $x_1^2,x_1x_2,x_2^2$ for <code>degree=2</code> and two features $x_1,x_2$.</p>"},{"location":"linear_models/linear_regression/#spline-regression","title":"Spline Regression","text":"<p>Spline regression can produce even more sophisticated relations by using a series of polynomial segments joining at specified knots. It is available with <code>SplineTransformer</code> from <code>sklearn.preprocessing</code>. The resulting segments can then be fit into regression model of one\u2019s choice (see more).</p>"},{"location":"linear_models/linear_regression/#generalized-linear-models","title":"Generalized Linear Models","text":"<p>The process of specifying knots in splines can be automated using generalized additive models (GAM) regression. GAMs are available in <code>pyGAM</code> package (see more).</p>"},{"location":"linear_models/regularization/","title":"Regularization","text":""},{"location":"linear_models/regularization/#biasvariance-trade-off","title":"Bias/Variance Trade-off","text":"<p>Model\u2019s generalization error can be represented as sum of three errors:</p> <ul> <li>[[base/Statistics/Notation#Bias|Bias]]</li> <li>[[base/Statistics/Notation#Variance|Variance]]</li> <li>Irreducible error is the part of generalization error due to the noise in the data.</li> </ul> <p>Increasing model\u2019s complexity typically decreases its bias and increases variance and vice versa.</p> <p>One approach to regularization is weight decay (i.e. penalizing the model for large weight values). It is used in the models below:</p> <ul> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Elastic Net</li> </ul> <p>Note</p> <p>Ridge Regression is a good default choice. Lasso and Elastic Net can be used instead when it\u2019s not obvious what features are the most important hence they both tend to eliminate useless features.</p>"},{"location":"linear_models/regularization/#ridge-regression","title":"Ridge Regression","text":"<p>Ridge regression (also called Tikhonov regularization) adds a regularization term equal to half square of the $\\ell_2$ norm of the weight vector to the $\\text{MSE}$ cost function</p> <p>$$ J(\\boldsymbol{w})=\\text{MSE}+\\frac{\\alpha}{2}|\\boldsymbol{w}|_2^2=\\text{MSE}+\\frac{\\alpha}{2}\\boldsymbol{w}^T\\boldsymbol{w} $$</p> <p>The normal equation thus changes to</p> <p>$$ \\boldsymbol{\\theta}=(\\boldsymbol{X}^T\\boldsymbol{X}+\\alpha\\boldsymbol{A})^{-1}\\boldsymbol{X}^T\\boldsymbol{y} $$</p> <p>The gradient vector of the cost function changes to</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})} = \\frac{2}{m}\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}) + \\alpha\\boldsymbol{w} $$</p> <ul> <li><code>sklearn.linear_model.Ridge</code></li> <li><code>sklearn.linear_model.SGDRegressor(penalty=\"l2\")</code></li> </ul>"},{"location":"linear_models/regularization/#lasso-regression","title":"Lasso Regression","text":"<p>Least Absolute Shrinkage and Selection Operator (LASSO) Regression adds a regularization term equal to $\\ell_1$ norm of the weight vector to the $\\text{MSE}$ cost function</p> <p>$$ J(\\boldsymbol{w})=\\text{MSE}+\\alpha|\\boldsymbol{w}|_1 $$</p> <p>Warning</p> <p>Unlike Ridge Regression, Lasso Regression tends to eliminate the weights of the least important features.</p> <ul> <li><code>sklearn.linear_model.Lasso</code></li> <li><code>sklearn.linear_model.SGDRegressor(penalty=\"l1\")</code></li> </ul>"},{"location":"linear_models/regularization/#elastic-net","title":"Elastic Net","text":"<p>Elastic net adds two regularization terms at once corresponding to $\\ell_1$ and $\\ell_2$ norms of the weight vector</p> <p>$$ J(\\boldsymbol{w})=\\text{MSE}+r\\alpha|\\boldsymbol{w}|_1+\\frac{1-r}{2}\\alpha|\\boldsymbol{w}|_2^2 $$</p> <p>where $r$ is the ratio between the terms (e.g. $r=1$ is equivalent to Lasso Regression, while $r=0$ is equivalent to Ridge Regression).</p> <p>Note</p> <p>Elastic Net\u2019s behaviour is similar to Lasso Regression.</p> <ul> <li><code>sklearn.linear_model.ElasticNet(l1_ratio=0.5)</code></li> <li><code>sklearn.linear_model.SGDRegressor(penalty=\"elasticnet\", l1_ratio=0.5)</code></li> </ul>"}]}